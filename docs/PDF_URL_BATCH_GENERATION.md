# PDF/URL 课程生成方案评估：下载缓存 + 分批生成

## 你的方案简述

- 当 URL 指向的是 PDF（或大文档）时：**先下载 PDF 并缓存**
- 再**按批（chunk）生成知识点**，每批一次 AI 调用，每批控制在 60s 内
- 最终得到多节微课 / 多个知识点，组成一门课

## 评估结论：**方向正确，是在不升级 Vercel 前提下较优的解**

| 维度 | 评价 |
|------|------|
| **是否解决 60s 限制** | ✅ 每批单独一次 serverless 调用，单次 <60s 即可 |
| **是否支持大 PDF** | ✅ 缓存后不必重复下载，可多次按批消费同一份内容 |
| **是否适配现有架构** | ✅ 可沿用现有「任务 + 轮询」：一个「PDF 课程任务」包含多批子步骤，前端轮询总任务或每批结果 |
| **实现复杂度** | ⚠️ 中等：需 PDF 检测、存储、分块策略、多节写入与任务状态机 |

整体上，**「PDF 下载 + 缓存 + 分批生成知识点」是在 Vercel 60s 限制下处理大 PDF 的合理且较优方案**。下面细化设计要点和可选替代方案。

---

## 1. 方案优点

- **单次调用时间可控**：每批只处理一段内容，容易把单次 AI 调用压在 60s 内。
- **支持大文档**：PDF 只下载一次，缓存后可被多批共享，避免重复拉取和超时。
- **可扩展**：后续可加「按章/按节」分块、进度展示（如 3/10 节）、失败重试单批等。
- **与现有能力一致**：项目已有 `pdf-parse`（上传解析），可复用于「URL 下载后的 PDF 解析」；`generation_jobs` 可扩展为「一个 job 多节 lesson」的批次任务。

---

## 2. 需要定好的设计点

### 2.1 缓存存哪、存多久

- **推荐**：**Supabase Storage**，按用户隔离（如 `pdf-cache/{user_id}/{hash(url)}.pdf`）。
- **Key**：`hash(url)`（如 SHA256 前 16 位），避免重复下载同一 URL。
- **TTL**：例如 7 天或 30 天，用 Object 的 `updated_at` 或单独元数据表清理；大文件可设单用户/全局容量上限。
- **备选**：不落盘，仅在「当前任务」生命周期内放在内存/临时文件，处理完即删（实现简单，但同一 URL 再次生成会重新下载）。

### 2.2 如何分块（chunk）—— 直接影响质量

- **按页**：实现简单，但可能把同一节/同一概念拆到两页，连贯性差。
- **按字数/token 数**：例如每块 2000–3000 字，带重叠（overlap 200 字），保证每批 <60s，但可能从段落中间切开。
- **按结构（推荐）**：  
  - 若有目录/标题：按「一级/二级标题」切段，一段 = 一批，知识点更完整。  
  - 若 PDF 解析不出结构：退化为「按页」或「按字数 + overlap」。
- **块大小**：单块控制在当前模型 60s 内能跑完（例如 2000 字以内 + `maxOutputTokens`），避免单批再超时。

### 2.3 PDF 解析：文字 + 公式 + 代码

- **现状**：`pdf-parse` 主要出纯文本，对公式、代码、表格的保留一般。
- **若需更好效果**：  
  - 公式：可接 Unstructured.io / LlamaParse 等，或先转 HTML/Markdown 再解析。  
  - 代码/表格：依赖解析器是否支持；否则仍以「文本为主 + 少量代码块」做第一批，再迭代。
- **建议**：V1 用 `pdf-parse` 拿纯文本 + 按块分批；V2 再考虑换解析或增强结构识别。

### 2.4 任务与结果模型

- **任务**：一个 URL/PDF = 一个「PDF 课程任务」（如 `generation_jobs` 扩展 type=`pdf_url`，或沿用 `url` 但增加 `batch_status`）。
- **状态**：例如 `pending → downloading → parsing → generating(3/10) → completed`，便于前端展示进度。
- **结果**：  
  - 每批生成 **一节** `generated_lessons`（同一 `user_course_id`），并更新该任务的「已生成节数」。  
  - 全部完成后，任务标记为 `completed`，前端跳转到「我的生成课程」或课程详情。

---

## 3. 和「不缓存、不分批」的对比

- **当前 URL 流**：拉 HTML → 截断 6000 字 → 一次生成一节。  
  - 问题：大 PDF 若转 HTML 再抓，往往拿不到全文；且单次 60s 容易超时。
- **只缓存、不分批**：下载 PDF 并解析成一大段文本，仍**一次**生成整门课。  
  - 问题：内容一大就容易超 60s，且模型容易截断或质量不稳。
- **你的方案（缓存 + 分批）**：既解决「大」又解决「单次时长」，是更优折中。

---

## 4. 其他可选方案（简要）

- **外部长时 worker**（如 Railway / Cloud Run）：一个进程跑 5–10 分钟，下载 + 解析 + 多批 AI 都在一台机器上完成，再写回 DB。  
  - 优点：逻辑简单，无 60s 限制。  
  - 缺点：要维护额外服务与部署，你当前倾向不升级 Vercel，这类方案成本略高。
- **队列 + 多步函数**（如 Inngest/Trigger.dev）：PDF 下载并写入 Storage 后，发 N 个「生成第 i 批」事件，由队列触发 N 次 Vercel 函数。  
  - 与「下载缓存 + 分批」思路一致，只是用队列显式拆步，可观测性更好，实现量略大。

在你**不升级 Vercel、不引入额外云服务**的前提下，**当前方案（下载 PDF → 缓存 → 分批生成）是最优主方向**。

---

## 5. 推荐实现顺序

1. **PDF 检测与下载**  
   - 若 `Content-Type` 为 `application/pdf` 或 URL 以 `.pdf` 结尾：走 PDF 分支；否则沿用现有 HTML 抓取。
2. **缓存**  
   - 下载到内存或临时文件，计算 `hash(url)`，写入 Supabase Storage（`pdf-cache/{user_id}/{hash}.pdf`），并记录元数据（url、大小、创建时间）。
3. **解析**  
   - 用现有 `pdf-parse` 从 Storage 或 buffer 解析出全文；若需结构，可尝试按「标题/页码」做简单分节。
4. **分块**  
   - 按节或按 2000–3000 字（+overlap）分块，每块对应「一批」。
5. **任务状态机**  
   - 一个 job：`downloading → parsing → generating`，`generating` 时维护 `current_batch/total_batches` 或已生成 lesson 数。
6. **分批调用 AI**  
   - 每批单独调一次 `generateLessonFromContent`（或等价接口），写入一节 `generated_lessons`，更新 job 进度；任一批失败可标记该批失败并继续或终止任务。
7. **前端**  
   - 轮询 job 状态，展示「已生成 3/10 节」；完成后跳转课程/列表。

这样可以在**不升级 Vercel** 的前提下，支持「URL 大 PDF + 文字/公式/代码」的完整流程，并保证每批在 60s 内完成、知识尽量完整。

---

## 6. 已实现（V1）

- **PDF 检测**：URL 以 `.pdf` 结尾或响应 `Content-Type: application/pdf` 时走 PDF 分支。
- **下载与解析**：`fetchUrlAsBuffer` + `pdf-parse` 解析为纯文本，按 2500 字/块、200 字重叠分块。
- **任务扩展**：`generation_jobs` 增加 `batches_total`、`batches_done`、`cached_chunks`（迁移 `20250227200000_generation_jobs_pdf_batches.sql`）。
- **分批执行**：第一次 process 下载 PDF、分块、创建课程、生成第 1 节并写入 `cached_chunks`；后续每次 process 生成下一节（由前端轮询后再次调用 process 触发）。
- **前端**：轮询时展示「已生成 x/y 节」；完成后多节显示「进入课程」跳转 `/learn/my/{userCourseId}`。
- **缓存**：V1 未落盘 PDF 文件，仅把分块存在 job 的 `cached_chunks` 中，避免重复解析；后续可加 Supabase Storage 缓存同一 URL。
